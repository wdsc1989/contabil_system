"""
Servi√ßo de IA para an√°lise inteligente de arquivos
"""
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
import json
import os
import re
from sqlalchemy.orm import Session

from config.ai_config import AIConfigManager


class AIService:
    """
    Servi√ßo para an√°lise de arquivos usando IA
    """
    
    # Tipos de dados suportados
    DATA_TYPES = {
        'transactions': 'Transa√ß√µes Financeiras',
        'bank_statements': 'Extratos Banc√°rios',
        'contracts': 'Contratos/Eventos',
        'accounts_payable': 'Contas a Pagar',
        'accounts_receivable': 'Contas a Receber',
        'financial_investments': 'Extratos de Aplica√ß√µes Financeiras',
        'credit_card_invoices': 'Faturas de Cart√£o de Cr√©dito',
        'card_machine_statements': 'Extratos de M√°quina de Cart√£o',
        'inventory': 'Controle de Estoque'
    }
    
    # Campos esperados por tipo
    EXPECTED_FIELDS = {
        'transactions': ['date', 'description', 'value'],
        'bank_statements': ['date', 'description', 'value'],
        'contracts': ['contract_start', 'event_date', 'service_value', 'contractor_name'],
        'accounts_payable': ['account_name', 'due_date', 'value'],
        'accounts_receivable': ['account_name', 'due_date', 'value'],
        'financial_investments': ['date', 'investment_type', 'applied_value', 'redeemed_value'],
        'credit_card_invoices': ['transaction_date', 'description', 'value'],
        'card_machine_statements': ['date', 'gross_value', 'net_value', 'transaction_type'],
        'inventory': ['product_name', 'quantity', 'unit_value', 'movement_date', 'movement_type']
    }

    def __init__(self, db: Session):
        """
        Inicializa o servi√ßo de IA com configura√ß√£o do banco
        """
        self.db = db
        self.config = AIConfigManager.get_config_dict(db)
        self._client = None

    def _reload_config(self):
        """
        Recarrega configura√ß√£o do banco de dados
        """
        self.config = AIConfigManager.get_config_dict(self.db)
        self._client = None  # Reseta cliente para recarregar com nova config

    def is_available(self) -> bool:
        """
        Verifica se o servi√ßo de IA est√° dispon√≠vel e configurado
        """
        return AIConfigManager.is_configured(self.db) and self.config is not None

    def _get_client(self):
        """
        Obt√©m cliente da API de IA baseado no provedor configurado
        Retorna (client, error_message) onde error_message √© None se sucesso
        """
        if not self.config:
            return None, "Configura√ß√£o de IA n√£o encontrada"
        
        if self._client is not None:
            return self._client, None
        
        provider = self.config['provider']
        api_key = self.config.get('api_key', '').strip()
        
        # Valida chave de API (exceto Ollama)
        if provider != 'ollama' and not api_key:
            return None, f"Chave de API n√£o configurada para {provider}"
        
        try:
            if provider == 'openai':
                try:
                    from openai import OpenAI
                except ImportError:
                    return None, "Biblioteca 'openai' n√£o instalada. Execute: pip install openai"
                self._client = OpenAI(api_key=api_key)
                return self._client, None
                
            elif provider == 'gemini':
                try:
                    import google.generativeai as genai
                except ImportError:
                    return None, "Biblioteca 'google-generativeai' n√£o instalada. Execute: pip install google-generativeai"
                genai.configure(api_key=api_key)
                model_name = self.config.get('model', 'gemini-1.5-flash')
                self._client = genai.GenerativeModel(model_name)
                return self._client, None
                
            elif provider == 'ollama':
                try:
                    from openai import OpenAI
                except ImportError:
                    return None, "Biblioteca 'openai' n√£o instalada. Execute: pip install openai"
                base_url = self.config.get('base_url', 'http://localhost:11434/v1')
                self._client = OpenAI(
                    api_key='ollama',  # Ollama n√£o requer chave real
                    base_url=base_url
                )
                return self._client, None
                
            elif provider == 'groq':
                try:
                    from groq import Groq
                except ImportError:
                    return None, "Biblioteca 'groq' n√£o instalada. Execute: pip install groq"
                self._client = Groq(api_key=api_key)
                return self._client, None
            else:
                return None, f"Provedor '{provider}' n√£o suportado"
                
        except Exception as e:
            error_msg = f"Erro ao inicializar cliente de IA ({provider}): {str(e)}"
            print(error_msg)
            return None, error_msg

    def _prepare_pdf_context(self, pdf_data: Dict[str, Any], import_type: str) -> str:
        """
        Prepara contexto adicional de PDF para incluir nos prompts da IA
        """
        context_parts = []
        
        # Metadados do PDF
        metadata = pdf_data.get('metadata', {})
        if metadata.get('title'):
            context_parts.append(f"- T√≠tulo do PDF: {metadata['title']}")
        if metadata.get('num_pages'):
            context_parts.append(f"- N√∫mero de p√°ginas: {metadata['num_pages']}")
        
        # Cabe√ßalhos e rodap√©s
        headers_footers = pdf_data.get('headers_footers', {})
        if headers_footers.get('header_text'):
            header_preview = headers_footers['header_text'][:500]  # Limita tamanho
            context_parts.append(f"- Cabe√ßalho do documento:\n{header_preview}")
        
        if headers_footers.get('bank_name'):
            context_parts.append(f"- Nome do banco detectado: {headers_footers['bank_name']}")
        
        if headers_footers.get('account_info'):
            context_parts.append(f"- Informa√ß√µes de conta: {headers_footers['account_info']}")
        
        # Texto completo (amostra das primeiras 2000 caracteres)
        full_text = pdf_data.get('full_text', '')
        if full_text:
            text_preview = full_text[:2000]  # Primeiros 2000 caracteres
            context_parts.append(f"- Texto do documento (amostra):\n{text_preview}")
            if len(full_text) > 2000:
                context_parts.append(f"- ... (texto completo tem {len(full_text)} caracteres)")
        
        return "\n".join(context_parts)
    
    def _text_to_dataframe(self, text: str) -> pd.DataFrame:
        """
        Tenta criar um DataFrame b√°sico a partir do texto do PDF
        Usa regex para identificar padr√µes de dados estruturados
        """
        lines = text.split('\n')
        records = []
        
        # Padr√µes comuns para identificar linhas de dados
        date_pattern = r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}'
        currency_pattern = r'R?\$?\s*-?\d{1,3}(?:[.,]\d{3})*(?:[.,]\d{2})?'
        
        for line in lines:
            line = line.strip()
            if not line or len(line) < 10:
                continue
            
            # Verifica se a linha parece ter dados estruturados
            has_date = bool(re.search(date_pattern, line))
            has_currency = bool(re.search(currency_pattern, line))
            
            if has_date or has_currency:
                # Tenta extrair campos
                date_match = re.search(date_pattern, line)
                currency_matches = re.findall(currency_pattern, line)
                
                record = {
                    'raw_text': line,
                    'date': date_match.group(0) if date_match else '',
                    'value': currency_matches[0] if currency_matches else '',
                    'description': line
                }
                records.append(record)
        
        if records:
            return pd.DataFrame(records)
        
        # Se n√£o encontrou padr√µes, cria DataFrame simples com o texto
        return pd.DataFrame({'text': lines[:100]})  # Limita a 100 linhas
    
    def _repair_json(self, json_str: str, error: json.JSONDecodeError) -> str:
        """
        Tenta reparar JSON malformado corrigindo problemas comuns
        """
        repaired = json_str
        
        # Obt√©m posi√ß√£o do erro
        error_pos = getattr(error, 'pos', None)
        error_msg = str(error)
        
        # Se o erro menciona v√≠rgula faltante, tenta adicionar
        if "Expecting ','" in error_msg or "delimiter" in error_msg:
            if error_pos and error_pos < len(repaired):
                # Analisa o contexto ao redor do erro (mais amplo para capturar o padr√£o)
                start = max(0, error_pos - 200)
                end = min(len(repaired), error_pos + 50)
                context = repaired[start:end]
                
                # Procura padr√µes comuns que indicam v√≠rgula faltante
                # Padr√£o: "valor" } ou "valor" ] ou n√∫mero } ou n√∫mero ]
                # Mas n√£o se j√° houver v√≠rgula antes
                before_error = repaired[:error_pos]
                after_error = repaired[error_pos:]
                
                # Verifica se precisa adicionar v√≠rgula antes de } ou ]
                # Procura por: valor seguido de } ou ] sem v√≠rgula
                # Procura de tr√°s para frente a partir do erro
                search_start = max(0, error_pos - 200)
                search_end = error_pos + 10
                search_area = repaired[search_start:search_end]
                
                # Padr√µes mais espec√≠ficos para detectar v√≠rgula faltante
                patterns_to_fix = [
                    # String seguida de } ou ] sem v√≠rgula
                    (r'("(?:[^"\\]|\\.)+")\s*([}\]])', r'\1, \2'),
                    # N√∫mero seguido de } ou ] sem v√≠rgula
                    (r'(\d+\.?\d*)\s*([}\]])', r'\1, \2'),
                    # Boolean/null seguido de } ou ] sem v√≠rgula
                    (r'\b(true|false|null)\b\s*([}\]])', r'\1, \2'),
                ]
                
                # Aplica os padr√µes na √°rea de busca
                fixed_area = search_area
                for pattern, replacement in patterns_to_fix:
                    # Aplica globalmente na √°rea, mas apenas se n√£o houver v√≠rgula antes
                    fixed_area = re.sub(pattern, replacement, fixed_area)
                
                if fixed_area != search_area:
                    repaired = repaired[:search_start] + fixed_area + repaired[search_end:]
        
        # Aplica corre√ß√µes globais de v√≠rgulas (mais agressivo)
        # Remove v√≠rgulas extras antes de } ou ]
        repaired = re.sub(r',\s*}', r'}', repaired)
        repaired = re.sub(r',\s*]', r']', repaired)
        
        # Remove v√≠rgulas duplicadas
        repaired = re.sub(r',\s*,', r',', repaired)
        
        # Tenta adicionar v√≠rgulas faltantes globalmente (mais conservador)
        # Apenas se n√£o houver v√≠rgula antes e n√£o for o primeiro item
        # Padr√£o: valor } ou valor ] onde valor n√£o √© seguido de v√≠rgula
        # Mas evita adicionar se j√° houver v√≠rgula antes
        repaired = re.sub(r'("(?:[^"\\]|\\.)+")\s+([}\]])', r'\1, \2', repaired)
        repaired = re.sub(r'(\d+\.?\d*)\s+([}\]])', r'\1, \2', repaired)
        repaired = re.sub(r'\b(true|false|null)\b\s+([}\]])', r'\1, \2', repaired)
        
        # Corrige strings n√£o terminadas
        if error_pos and error_pos < len(repaired):
            before_error = repaired[:error_pos]
            # Conta aspas n√£o escapadas
            quote_count = 0
            escape_next = False
            for char in before_error:
                if escape_next:
                    escape_next = False
                    continue
                if char == '\\':
                    escape_next = True
                    continue
                if char == '"':
                    quote_count += 1
            
            if quote_count % 2 != 0:
                # String n√£o terminada, tenta fechar
                after_error = repaired[error_pos:]
                next_quote = after_error.find('"')
                next_brace = after_error.find('}')
                next_bracket = after_error.find(']')
                
                if next_quote != -1 and (next_brace == -1 or next_quote < next_brace) and (next_bracket == -1 or next_quote < next_bracket):
                    repaired = repaired[:error_pos] + '"' + repaired[error_pos:]
                elif next_brace != -1:
                    repaired = repaired[:error_pos] + '"' + repaired[error_pos:]
        
        # Remove caracteres de controle
        repaired = re.sub(r'[\x00-\x1f\x7f-\x9f]', ' ', repaired)
        
        return repaired
    
    def _extract_partial_json(self, json_str: str) -> Optional[Dict[str, Any]]:
        """
        Tenta extrair dados parciais de JSON malformado
        """
        try:
            # Tenta encontrar e extrair apenas o processed_data
            # Usa padr√£o mais flex√≠vel para encontrar o array
            processed_data_match = re.search(r'"processed_data"\s*:\s*\[', json_str)
            if not processed_data_match:
                return None
            
            # Encontra o in√≠cio do array
            array_start = processed_data_match.end()
            
            # Procura por objetos { ... } dentro do array
            objects = []
            depth = 0
            array_depth = 1  # Profundidade do array
            current_obj = ""
            in_string = False
            escape_next = False
            obj_start = -1
            
            for i in range(array_start, len(json_str)):
                char = json_str[i]
                
                if escape_next:
                    if obj_start >= 0:
                        current_obj += char
                    escape_next = False
                    continue
                
                if char == '\\':
                    if obj_start >= 0:
                        current_obj += char
                    escape_next = True
                    continue
                
                if char == '"':
                    in_string = not in_string
                    if obj_start >= 0:
                        current_obj += char
                elif not in_string:
                    if char == '[':
                        array_depth += 1
                        if obj_start >= 0:
                            current_obj += char
                    elif char == ']':
                        array_depth -= 1
                        if array_depth == 0:
                            # Fim do array, tenta salvar √∫ltimo objeto se houver
                            if obj_start >= 0 and depth == 0 and current_obj:
                                try:
                                    obj = json.loads(current_obj)
                                    objects.append(obj)
                                except:
                                    pass
                            break
                        elif obj_start >= 0:
                            current_obj += char
                    elif char == '{':
                        if depth == 0:
                            obj_start = i
                            current_obj = "{"
                        else:
                            current_obj += char
                        depth += 1
                    elif char == '}':
                        if obj_start >= 0:
                            current_obj += char
                        depth -= 1
                        if depth == 0 and obj_start >= 0:
                            # Objeto completo encontrado
                            try:
                                obj = json.loads(current_obj)
                                objects.append(obj)
                            except json.JSONDecodeError:
                                # Tenta reparar o objeto antes de parsear
                                try:
                                    repaired_obj = self._repair_json(current_obj, json.JSONDecodeError("", current_obj, 0))
                                    obj = json.loads(repaired_obj)
                                    objects.append(obj)
                                except:
                                    pass
                            current_obj = ""
                            obj_start = -1
                    else:
                        if obj_start >= 0:
                            current_obj += char
                else:
                    if obj_start >= 0:
                        current_obj += char
            
            # Constr√≥i resultado parcial
            if objects:
                result = {
                    'processed_data': objects,
                    'summary': {},
                    'issues': []
                }
                
                # Tenta extrair summary
                summary_match = re.search(r'"summary"\s*:\s*\{([^}]*)\}', json_str)
                if summary_match:
                    try:
                        summary_str = "{" + summary_match.group(1) + "}"
                        summary = json.loads(summary_str)
                        result['summary'] = summary
                    except:
                        # Tenta criar summary b√°sico
                        result['summary'] = {
                            'total_rows': len(objects),
                            'processed': len(objects),
                            'errors': 0
                        }
                
                # Tenta extrair issues
                issues_match = re.search(r'"issues"\s*:\s*\[(.*?)\]', json_str, re.DOTALL)
                if issues_match:
                    try:
                        issues_str = "[" + issues_match.group(1) + "]"
                        issues = json.loads(issues_str)
                        result['issues'] = issues
                    except:
                        result['issues'] = ["Alguns dados podem ter sido perdidos devido a erro de parsing"]
                
                return result
        except Exception as e:
            print(f"Erro ao extrair JSON parcial: {e}")
        
        return None
    
    def _prepare_data_sample(self, df: pd.DataFrame, max_rows: int = 5) -> str:
        """
        Prepara amostra dos dados para an√°lise
        """
        sample = df.head(max_rows)
        
        # Converte para formato leg√≠vel
        data_info = {
            'columns': list(df.columns),
            'total_rows': len(df),
            'sample_data': sample.to_dict('records')
        }
        
        return json.dumps(data_info, indent=2, default=str, ensure_ascii=False)

    def _create_prompt_for_validation(
        self,
        columns: List[str],
        data_sample: str,
        selected_type: str
    ) -> str:
        """
        Cria prompt para valida√ß√£o do tipo de dados escolhido
        """
        expected_fields = self.EXPECTED_FIELDS.get(selected_type, [])
        type_name = self.DATA_TYPES.get(selected_type, selected_type)
        
        prompt = f"""Voc√™ √© um assistente especializado em an√°lise de dados financeiros e cont√°beis.

Analise o arquivo fornecido e determine se o tipo de dados selecionado pelo usu√°rio faz sentido.

**Tipo de dados selecionado:** {type_name}
**Campos esperados para este tipo:** {', '.join(expected_fields)}

**Colunas do arquivo:**
{', '.join(columns)}

**Amostra dos dados (primeiras 5 linhas):**
{data_sample}

**Tarefa:**
1. Analise se as colunas e dados do arquivo s√£o compat√≠veis com o tipo "{type_name}"
2. Identifique quais campos esperados est√£o presentes ou podem ser mapeados
3. Avalie a compatibilidade geral

**Responda em formato JSON:**
{{
    "compatible": true/false,
    "confidence": 0.0-1.0,
    "reason": "explica√ß√£o breve",
    "missing_fields": ["lista de campos faltantes"],
    "found_fields": ["lista de campos encontrados"],
    "suggestions": "sugest√µes de melhoria ou tipo alternativo"
}}
"""
        return prompt

    def _create_prompt_for_mapping(
        self,
        columns: List[str],
        data_sample: str,
        import_type: str
    ) -> str:
        """
        Cria prompt para sugest√£o de mapeamento de colunas
        """
        expected_fields = self.EXPECTED_FIELDS.get(import_type, [])
        type_name = self.DATA_TYPES.get(import_type, import_type)
        
        prompt = f"""Voc√™ √© um assistente especializado em mapeamento de colunas de dados financeiros.

Analise o arquivo e sugira o melhor mapeamento das colunas do arquivo para os campos do sistema.

**Tipo de dados:** {type_name}
**Campos esperados pelo sistema:** {', '.join(expected_fields)}

**Colunas do arquivo:**
{', '.join(columns)}

**Amostra dos dados (primeiras 5 linhas):**
{data_sample}

**Tarefa:**
Para cada coluna do arquivo, sugira o campo do sistema mais apropriado.
Se uma coluna n√£o se encaixa em nenhum campo, sugira "ignore".

**Responda em formato JSON:**
{{
    "mapping": {{
        "nome_coluna_arquivo": "campo_sistema",
        ...
    }},
    "confidence": 0.0-1.0,
    "notes": "observa√ß√µes sobre o mapeamento"
}}
"""
        return prompt

    def _call_ai(
        self, 
        prompt: str, 
        model: Optional[str] = None,
        status_callback: Optional[callable] = None
    ) -> Tuple[Optional[str], Optional[str]]:
        """
        Chama a API de IA e retorna (resposta, erro)
        Se erro for None, resposta cont√©m o texto retornado
        
        Args:
            prompt: Prompt para enviar √† IA
            model: Nome do modelo (opcional)
            status_callback: Fun√ß√£o callback(status_message) para atualizar status em tempo real
        """
        if status_callback:
            status_callback("Conectando √† API de IA...")
        
        client, error = self._get_client()
        if error:
            return None, error
        
        if not client:
            return None, "Cliente de IA n√£o inicializado"
        
        provider = self.config['provider']
        model_name = model or self.config.get('model')
        
        if not model_name:
            return None, "Nome do modelo n√£o configurado"
        
        try:
            if status_callback:
                status_callback(f"Enviando requisi√ß√£o para {provider} (modelo: {model_name})...")
            
            if provider == 'openai' or provider == 'ollama' or provider == 'groq':
                system_message = "Voc√™ √© um assistente especializado em an√°lise de dados financeiros e cont√°beis. Sempre responda APENAS em formato JSON v√°lido, sem texto adicional antes ou depois do JSON."
                user_message = prompt
                
                if status_callback:
                    status_callback("Processando com IA...")
                
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": system_message},
                        {"role": "user", "content": user_message}
                    ],
                    temperature=0.2,  # Reduzido para respostas mais r√°pidas e consistentes
                    max_tokens=6000,  # Otimizado: reduzido de 8000 para melhor performance
                    response_format={"type": "json_object"} if provider == 'openai' else None
                )
                
                if status_callback:
                    status_callback("Recebendo resposta da IA...")
                
                if response and response.choices and len(response.choices) > 0:
                    return response.choices[0].message.content, None
                else:
                    return None, "Resposta vazia da API"
            
            elif provider == 'gemini':
                if status_callback:
                    status_callback("Processando com Gemini...")
                
                # Gemini precisa do system message no prompt
                full_prompt = f"""Voc√™ √© um assistente especializado em an√°lise de dados financeiros e cont√°beis. Sempre responda APENAS em formato JSON v√°lido, sem texto adicional antes ou depois do JSON.

{prompt}"""
                
                response = client.generate_content(full_prompt)
                
                if status_callback:
                    status_callback("Recebendo resposta da IA...")
                
                if response and response.text:
                    return response.text, None
                else:
                    return None, "Resposta vazia da API"
            
            else:
                return None, f"Provedor '{provider}' n√£o suportado"
            
        except Exception as e:
            error_msg = f"Erro ao chamar API de IA ({provider}): {str(e)}"
            print(error_msg)
            return None, error_msg

    def validate_data_type(
        self,
        df: pd.DataFrame,
        selected_type: str
    ) -> Dict[str, Any]:
        """
        Valida se o tipo de dados escolhido faz sentido para o arquivo
        
        Retorna:
        {
            'compatible': bool,
            'confidence': float (0-1),
            'reason': str,
            'missing_fields': List[str],
            'found_fields': List[str],
            'suggestions': str
        }
        """
        if not self.is_available():
            return {
                'compatible': True,  # Assume compat√≠vel se IA n√£o dispon√≠vel
                'confidence': 0.0,
                'reason': 'IA n√£o configurada',
                'missing_fields': [],
                'found_fields': [],
                'suggestions': ''
            }
        
        try:
            columns = list(df.columns)
            data_sample = self._prepare_data_sample(df)
            prompt = self._create_prompt_for_validation(columns, data_sample, selected_type)
            
            response, error = self._call_ai(prompt)
            
            if error:
                # Retorna erro espec√≠fico
                return {
                    'compatible': True,
                    'confidence': 0.0,
                    'reason': f'Erro na IA: {error}',
                    'missing_fields': [],
                    'found_fields': columns,
                    'suggestions': '',
                    'error': error
                }
            
            if response:
                # Tenta extrair JSON da resposta
                try:
                    # Remove markdown code blocks se existirem
                    response_clean = response
                    if '```json' in response_clean:
                        response_clean = response_clean.split('```json')[1].split('```')[0]
                    elif '```' in response_clean:
                        response_clean = response_clean.split('```')[1].split('```')[0]
                    
                    result = json.loads(response_clean.strip())
                    return result
                except json.JSONDecodeError as e:
                    # Se n√£o conseguir parsear, retorna erro
                    return {
                        'compatible': True,
                        'confidence': 0.5,
                        'reason': f'Resposta da IA n√£o p√¥de ser parseada como JSON: {str(e)}',
                        'missing_fields': [],
                        'found_fields': columns,
                        'suggestions': response[:200] if response else '',
                        'error': f'Erro ao parsear JSON: {str(e)}',
                        'raw_response': response[:500] if response else ''
                    }
            else:
                return {
                    'compatible': True,
                    'confidence': 0.0,
                    'reason': 'Sem resposta da IA',
                    'missing_fields': [],
                    'found_fields': columns,
                    'suggestions': '',
                    'error': 'Sem resposta da API'
                }
        except Exception as e:
            error_msg = f"Erro ao validar tipo de dados: {str(e)}"
            print(error_msg)
            return {
                'compatible': True,
                'confidence': 0.0,
                'reason': error_msg,
                'missing_fields': [],
                'found_fields': list(df.columns),
                'suggestions': '',
                'error': error_msg
            }

    def suggest_column_mapping(
        self,
        df: pd.DataFrame,
        import_type: str,
        target_columns: List[str]
    ) -> Dict[str, str]:
        """
        Sugere mapeamento de colunas usando IA
        
        Retorna dicion√°rio: {coluna_arquivo: campo_sistema}
        """
        if not self.is_available():
            return {}
        
        try:
            columns = list(df.columns)
            data_sample = self._prepare_data_sample(df)
            prompt = self._create_prompt_for_mapping(columns, data_sample, import_type)
            
            response, error = self._call_ai(prompt)
            
            if error:
                print(f"Erro ao obter mapeamento da IA: {error}")
                return {}
            
            if response:
                try:
                    # Remove markdown code blocks se existirem
                    response_clean = response
                    if '```json' in response_clean:
                        response_clean = response_clean.split('```json')[1].split('```')[0]
                    elif '```' in response_clean:
                        response_clean = response_clean.split('```')[1].split('```')[0]
                    
                    result = json.loads(response_clean.strip())
                    mapping = result.get('mapping', {})
                    
                    # Valida que os campos mapeados existem em target_columns
                    validated_mapping = {}
                    for source_col, target_col in mapping.items():
                        if source_col in columns:
                            if target_col in target_columns or target_col == 'ignore':
                                validated_mapping[source_col] = target_col
                            else:
                                validated_mapping[source_col] = 'ignore'
                    
                    return validated_mapping
                except json.JSONDecodeError as e:
                    print(f"Erro ao parsear JSON do mapeamento: {e}")
                    print(f"Resposta recebida: {response[:500]}")
                    return {}
            else:
                print("Sem resposta da IA para mapeamento")
                return {}
        except Exception as e:
            print(f"Erro ao sugerir mapeamento: {e}")
            return {}

    def test_connection(self) -> Tuple[bool, str]:
        """
        Testa conex√£o com a API de IA
        
        Retorna: (sucesso, mensagem)
        """
        if not self.config:
            return False, "IA n√£o configurada"
        
        try:
            # Recarrega cliente para garantir que est√° atualizado
            self._client = None
            client, error = self._get_client()
            
            if error:
                return False, error
            
            if not client:
                return False, "Erro ao inicializar cliente de IA"
            
            # Teste simples
            test_prompt = "Responda apenas: OK"
            response, error = self._call_ai(test_prompt)
            
            if error:
                return False, error
            
            if response:
                return True, f"Conex√£o bem-sucedida! Resposta: {response[:50]}"
            else:
                return False, "Sem resposta da API"
        except Exception as e:
            return False, f"Erro: {str(e)}"

    def _create_prompt_structural_analysis(
        self,
        columns: List[str],
        data_sample: str,
        import_type: str
    ) -> str:
        """
        Cria prompt para an√°lise estrutural completa do arquivo
        """
        expected_fields = self.EXPECTED_FIELDS.get(import_type, [])
        optional_fields = self.get_target_columns(import_type)
        optional_fields = [f for f in optional_fields if f not in expected_fields]
        type_name = self.DATA_TYPES.get(import_type, import_type)
        
        prompt = f"""Voc√™ √© um especialista em an√°lise de dados financeiros e cont√°beis.

Analise o arquivo fornecido e forne√ßa uma an√°lise estrutural completa.

**Colunas do arquivo:**
{', '.join(columns)}

**Amostra dos dados (primeiras 10 linhas):**
{data_sample}

**Tipo de dados esperado:** {type_name}
**Campos obrigat√≥rios:** {', '.join(expected_fields)}
**Campos opcionais:** {', '.join(optional_fields) if optional_fields else 'Nenhum'}

**Tarefa:**
1. Identifique o tipo real de dados no arquivo
2. Analise cada coluna: tipo de dado, formato, padr√µes, valores √∫nicos
3. Identifique campos de data e seus formatos
4. Identifique campos monet√°rios e seus formatos (R$, BRL, etc)
5. Identifique campos de descri√ß√£o/hist√≥rico
6. Detecte campos que podem ser inferidos ou calculados
7. Identifique inconsist√™ncias ou problemas nos dados
8. Sugira transforma√ß√µes necess√°rias

**Responda em formato JSON:**
{{
    "file_type": "tipo identificado",
    "columns_analysis": {{
        "nome_coluna": {{
            "type": "date|currency|text|numeric|boolean",
            "format": "formato espec√≠fico se aplic√°vel",
            "sample_values": ["valor1", "valor2"],
            "patterns": "padr√µes identificados",
            "issues": ["problemas encontrados"],
            "suggested_mapping": "campo_sistema_sugerido"
        }}
    }},
    "data_quality": {{
        "completeness": 0.0-1.0,
        "consistency": 0.0-1.0,
        "issues": ["lista de problemas"]
    }},
    "transformations_needed": [
        {{
            "column": "nome_coluna",
            "transformation": "normalize_date|normalize_currency|extract_text|etc",
            "from_format": "formato atual",
            "to_format": "formato desejado"
        }}
    ],
    "inferred_fields": {{
        "campo_sistema": {{
            "source": "coluna_origem ou calculado",
            "method": "como foi inferido",
            "confidence": 0.0-1.0
        }}
    }}
}}
"""
        return prompt

    def _create_prompt_detect_type(
        self,
        columns: List[str],
        data_sample: str
    ) -> str:
        """
        Cria prompt para detec√ß√£o autom√°tica do tipo de dado
        """
        prompt = f"""Voc√™ √© um especialista em an√°lise de dados financeiros e cont√°beis.

Analise o arquivo fornecido e identifique automaticamente qual tipo de dado ele cont√©m.

**Tipos de dados suportados e suas caracter√≠sticas:**

1. **üí≥ Transa√ß√µes Financeiras (transactions):**
   - Campos t√≠picos: data, descri√ß√£o, valor, tipo (entrada/saida), categoria
   - Valores podem ser positivos ou negativos
   - Geralmente tem coluna de tipo ou sinal
   - Pode ter categoria, conta, grupo

2. **üè¶ Extratos Banc√°rios (bank_statements):**
   - Campos t√≠picos: data, hist√≥rico/descri√ß√£o, valor, saldo, banco, conta
   - Valores podem ser positivos (cr√©dito) ou negativos (d√©bito)
   - Geralmente tem saldo acumulado
   - Pode ter informa√ß√µes de banco e conta
   - Padr√µes: "TED", "DOC", "PIX", "SALDO", "EXTRATO"

3. **üìù Contratos/Eventos (contracts):**
   - Campos t√≠picos: data_inicio, data_evento, valor_servico, contratante, tipo_evento
   - Geralmente tem datas de in√≠cio e evento
   - Tem valor de servi√ßo e poss√≠vel valor de deslocamento
   - Tem nome do contratante/cliente

4. **üí∏ Contas a Pagar (accounts_payable):**
   - Campos t√≠picos: conta, vencimento, valor, cpf_cnpj, mes_referencia
   - Tem data de vencimento
   - Geralmente tem nome da conta/fornecedor
   - Pode ter status de pagamento

5. **üí∞ Contas a Receber (accounts_receivable):**
   - Campos t√≠picos: conta, vencimento, valor, cpf_cnpj, mes_referencia
   - Tem data de vencimento
   - Geralmente tem nome do devedor/cliente
   - Pode ter status de recebimento

**Colunas encontradas no arquivo:**
{', '.join(columns)}

**Amostra dos dados (primeiras 15 linhas):**
{data_sample}

**Tarefa:**
Analise as colunas e a amostra de dados para identificar qual tipo de dado o arquivo cont√©m.
Compare os padr√µes encontrados com as caracter√≠sticas de cada tipo.
Retorne o tipo mais prov√°vel com n√≠vel de confian√ßa e justificativa.

**Responda em formato JSON:**
{{
    "suggested_type": "transactions" | "bank_statements" | "contracts" | "accounts_payable" | "accounts_receivable",
    "confidence": 0.0-1.0,
    "reasoning": "explica√ß√£o detalhada do motivo",
    "alternative_types": [
        {{
            "type": "bank_statements",
            "confidence": 0.3,
            "reason": "motivo da alternativa"
        }}
    ],
    "detected_fields": {{
        "date": "nome_coluna_encontrada",
        "value": "nome_coluna_encontrada",
        "description": "nome_coluna_encontrada"
    }},
    "key_indicators": [
        "indicador 1 que levou √† conclus√£o",
        "indicador 2 que levou √† conclus√£o"
    ]
}}
"""
        return prompt

    def detect_data_type(
        self,
        df: pd.DataFrame,
        columns: List[str],
        data_sample: str
    ) -> Dict[str, Any]:
        """
        Detecta automaticamente o tipo de dado do arquivo usando IA
        
        Retorna:
        {
            'success': bool,
            'suggested_type': str,
            'confidence': float,
            'reasoning': str,
            'alternative_types': List[Dict],
            'detected_fields': Dict,
            'error': str
        }
        """
        if not self.is_available():
            return {
                'success': False,
                'error': 'IA n√£o configurada ou n√£o dispon√≠vel',
                'suggested_type': None,
                'confidence': 0.0,
                'reasoning': '',
                'alternative_types': [],
                'detected_fields': {}
            }
        
        try:
            prompt = self._create_prompt_detect_type(columns, data_sample)
            response, error = self._call_ai(prompt)
            
            if error:
                return {
                    'success': False,
                    'error': error,
                    'suggested_type': None,
                    'confidence': 0.0,
                    'reasoning': '',
                    'alternative_types': [],
                    'detected_fields': {}
                }
            
            if not response:
                return {
                    'success': False,
                    'error': 'Sem resposta da IA',
                    'suggested_type': None,
                    'confidence': 0.0,
                    'reasoning': '',
                    'alternative_types': [],
                    'detected_fields': {}
                }
            
            # Parse da resposta
            try:
                # Remove markdown code blocks se existirem
                if '```json' in response:
                    response = response.split('```json')[1].split('```')[0]
                elif '```' in response:
                    response = response.split('```')[1].split('```')[0]
                
                # Limpa a resposta
                response_clean = response.strip()
                
                # Tenta encontrar o JSON v√°lido na resposta
                start_idx = response_clean.find('{')
                if start_idx == -1:
                    raise json.JSONDecodeError("JSON n√£o encontrado", response_clean, 0)
                
                end_idx = response_clean.rfind('}')
                if end_idx == -1 or end_idx <= start_idx:
                    raise json.JSONDecodeError("JSON incompleto", response_clean, start_idx)
                
                json_str = response_clean[start_idx:end_idx + 1]
                result = json.loads(json_str)
                
                return {
                    'success': True,
                    'suggested_type': result.get('suggested_type'),
                    'confidence': float(result.get('confidence', 0.0)),
                    'reasoning': result.get('reasoning', ''),
                    'alternative_types': result.get('alternative_types', []),
                    'detected_fields': result.get('detected_fields', {}),
                    'key_indicators': result.get('key_indicators', []),
                    'error': None
                }
                
            except json.JSONDecodeError as e:
                return {
                    'success': False,
                    'error': f'Erro ao parsear resposta da IA: {str(e)}',
                    'suggested_type': None,
                    'confidence': 0.0,
                    'reasoning': '',
                    'alternative_types': [],
                    'detected_fields': {},
                    'raw_response': response[:500]
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': f'Erro ao detectar tipo de dado: {str(e)}',
                'suggested_type': None,
                'confidence': 0.0,
                'reasoning': '',
                'alternative_types': [],
                'detected_fields': {}
            }

    def _create_prompt_normalization(
        self,
        file_data: str,
        import_type: str,
        structural_analysis: str,
        mapping: Dict[str, str]
    ) -> str:
        """
        Cria prompt para normaliza√ß√£o e estrutura√ß√£o de dados
        """
        type_name = self.DATA_TYPES.get(import_type, import_type)
        expected_fields = self.EXPECTED_FIELDS.get(import_type, [])
        all_fields = self.get_target_columns(import_type)
        
        expected_structure = {
            field: "tipo e formato esperado" for field in all_fields
        }
        
        prompt = f"""Voc√™ √© um especialista em normaliza√ß√£o e estrutura√ß√£o de dados financeiros.

Normalize e estruture os dados do arquivo para o formato esperado pelo sistema.

**Tipo de dados:** {type_name}
**Estrutura esperada:**
{json.dumps(expected_structure, indent=2, ensure_ascii=False)}

**Mapeamento de colunas:**
{json.dumps(mapping, indent=2, ensure_ascii=False)}

**Dados do arquivo (primeiras 20 linhas):**
{file_data}

**An√°lise estrutural pr√©via:**
{structural_analysis}

**Tarefa:**
Para cada linha de dados:
1. Normalize datas para formato YYYY-MM-DD
2. Normalize valores monet√°rios para n√∫mero decimal (sem s√≠mbolos)
3. Identifique tipo de transa√ß√£o (entrada/sa√≠da) baseado em valores ou descri√ß√µes
4. Extraia informa√ß√µes relevantes de campos de texto
5. Preencha campos faltantes com valores inferidos quando poss√≠vel
6. Valide e corrija dados inconsistentes
7. Estruture no formato JSON esperado

**Regras de normaliza√ß√£o:**
- Datas: Converter qualquer formato para YYYY-MM-DD
- Valores: Remover s√≠mbolos (R$, BRL, etc), pontos de milhar, manter apenas v√≠rgula ou ponto decimal
- Descri√ß√µes: Limpar espa√ßos extras, normalizar caracteres
- Tipos: Identificar automaticamente entrada/sa√≠da baseado em sinais ou palavras-chave

**Responda em formato JSON com array de objetos normalizados:**
{{
    "normalized_data": [
        {{
            "date": "YYYY-MM-DD",
            "description": "descri√ß√£o normalizada",
            "value": 1234.56,
            "type": "entrada|saida",
            "category": "categoria se identificada",
            "account": "conta se identificada",
            "original_row": 1,
            "transformations_applied": ["lista de transforma√ß√µes"],
            "confidence": 0.0-1.0
        }}
    ],
    "summary": {{
        "total_rows": 100,
        "successfully_normalized": 95,
        "rows_with_issues": 5,
        "common_issues": ["lista de problemas comuns"]
    }}
}}
"""
        return prompt

    def _create_prompt_validation(
        self,
        normalized_data: str,
        import_type: str
    ) -> str:
        """
        Cria prompt para valida√ß√£o e corre√ß√£o inteligente
        """
        type_name = self.DATA_TYPES.get(import_type, import_type)
        required_fields = self.EXPECTED_FIELDS.get(import_type, [])
        
        validation_rules = {
            "required_fields": required_fields,
            "date_format": "YYYY-MM-DD",
            "value_format": "n√∫mero decimal",
            "type_values": ["entrada", "saida"]
        }
        
        prompt = f"""Voc√™ √© um especialista em valida√ß√£o e corre√ß√£o de dados financeiros.

Valide e corrija os dados normalizados, garantindo consist√™ncia e completude.

**Tipo de dados:** {type_name}
**Dados normalizados:**
{normalized_data}

**Regras de valida√ß√£o:**
{json.dumps(validation_rules, indent=2, ensure_ascii=False)}

**Tarefa:**
1. Valide cada registro:
   - Datas est√£o no formato correto e s√£o v√°lidas?
   - Valores s√£o num√©ricos v√°lidos?
   - Descri√ß√µes n√£o est√£o vazias?
   - Campos obrigat√≥rios est√£o preenchidos?
2. Identifique inconsist√™ncias:
   - Valores muito altos ou muito baixos (outliers)
   - Datas fora de per√≠odo esperado
   - Descri√ß√µes duplicadas suspeitas
   - Padr√µes an√¥malos
3. Corrija problemas quando poss√≠vel:
   - Corrija formatos de data
   - Corrija valores monet√°rios
   - Complete campos faltantes com valores inferidos
   - Sugira corre√ß√µes para dados inv√°lidos
4. Classifique registros:
   - V√°lido e pronto para importa√ß√£o
   - V√°lido com avisos
   - Inv√°lido mas corrig√≠vel
   - Inv√°lido e n√£o corrig√≠vel

**Responda em formato JSON:**
{{
    "validated_data": [
        {{
            "row": 1,
            "data": {{dados validados}},
            "status": "valid|warning|error",
            "issues": ["problemas encontrados"],
            "corrections": ["corre√ß√µes aplicadas"],
            "confidence": 0.0-1.0
        }}
    ],
    "validation_summary": {{
        "total": 100,
        "valid": 90,
        "with_warnings": 5,
        "with_errors": 5,
        "corrected": 3
    }},
    "recommendations": [
        "recomenda√ß√µes para melhorar qualidade dos dados"
    ]
}}
"""
        return prompt

    def _create_prompt_inference(
        self,
        available_data: str,
        import_type: str,
        missing_fields: List[str],
        context: Optional[str] = None
    ) -> str:
        """
        Cria prompt para infer√™ncia de campos faltantes
        """
        type_name = self.DATA_TYPES.get(import_type, import_type)
        
        prompt = f"""Voc√™ √© um especialista em infer√™ncia de dados financeiros.

Analise os dados e infira campos faltantes baseado em padr√µes, contexto e regras de neg√≥cio.

**Tipo de dados:** {type_name}
**Dados dispon√≠veis:**
{available_data}

**Campos faltantes que precisam ser inferidos:**
{', '.join(missing_fields)}

**Contexto adicional:**
{context or 'Nenhum contexto adicional fornecido'}

**Tarefa:**
Para cada campo faltante:
1. Analise se pode ser inferido dos dados existentes
2. Identifique padr√µes que permitam infer√™ncia
3. Aplique regras de neg√≥cio conhecidas
4. Calcule valores quando poss√≠vel
5. Classifique por tipo de transa√ß√£o/descri√ß√£o quando aplic√°vel
6. Atribua valores padr√£o quando necess√°rio

**Exemplos de infer√™ncias:**
- Tipo (entrada/sa√≠da): baseado em sinal do valor ou palavras-chave na descri√ß√£o
- Categoria: baseado em palavras-chave na descri√ß√£o
- Conta: baseado em padr√µes de descri√ß√£o ou valores
- Data: inferir de outros campos de data se dispon√≠vel

**Responda em formato JSON:**
{{
    "inferred_fields": {{
        "campo1": {{
            "method": "como foi inferido",
            "source": "fonte dos dados",
            "value": "valor inferido",
            "confidence": 0.0-1.0,
            "rules_applied": ["regras aplicadas"]
        }}
    }},
    "inference_summary": {{
        "fields_inferred": ["lista de campos"],
        "average_confidence": 0.0-1.0,
        "methods_used": ["m√©todos utilizados"]
    }}
}}
"""
        return prompt

    def _create_prompt_intelligent_mapping(
        self,
        columns: List[str],
        data_sample: str,
        import_type: str,
        structural_analysis: Optional[str] = None
    ) -> str:
        """
        Cria prompt para mapeamento inteligente com contexto
        """
        expected_fields = self.EXPECTED_FIELDS.get(import_type, [])
        all_fields = self.get_target_columns(import_type)
        type_name = self.DATA_TYPES.get(import_type, import_type)
        
        prompt = f"""Voc√™ √© um especialista em mapeamento inteligente de dados financeiros.

Mapeie as colunas do arquivo para os campos do sistema considerando nomes, conte√∫do, formato e contexto.

**Colunas do arquivo:**
{', '.join(columns)}

**Amostra de dados (primeiras 15 linhas):**
{data_sample}

**Tipo de dados:** {type_name}
**Campos esperados pelo sistema:**
{', '.join(all_fields)}
**Campos obrigat√≥rios:** {', '.join(expected_fields)}

**An√°lise estrutural:**
{structural_analysis or 'N√£o dispon√≠vel'}

**Tarefa:**
1. Para cada campo esperado, identifique a melhor coluna correspondente considerando:
   - Nome da coluna (similaridade sem√¢ntica)
   - Tipo e formato dos dados
   - Padr√µes nos valores
   - Contexto e posi√ß√£o na estrutura
2. Para colunas que n√£o mapeiam diretamente:
   - Identifique se podem ser combinadas
   - Identifique se precisam de transforma√ß√£o
   - Identifique se devem ser ignoradas
3. Crie regras de transforma√ß√£o quando necess√°rio
4. Valide o mapeamento proposto

**Responda em formato JSON:**
{{
    "mapping": {{
        "coluna_arquivo": {{
            "target_field": "campo_sistema",
            "transformation": "nenhuma|normalize_date|normalize_currency|extract_text|combine|etc",
            "confidence": 0.0-1.0,
            "reason": "por que este mapeamento foi escolhido"
        }}
    }},
    "transformation_rules": [
        {{
            "column": "coluna",
            "rule": "regra de transforma√ß√£o",
            "example": "exemplo de transforma√ß√£o"
        }}
    ],
    "missing_fields": {{
        "campo": {{
            "can_infer": true/false,
            "method": "como inferir",
            "confidence": 0.0-1.0
        }}
    }},
    "mapping_confidence": 0.0-1.0
}}
"""
        return prompt

    def get_target_columns(self, import_type: str) -> List[str]:
        """
        Retorna todas as colunas alvo (obrigat√≥rias + opcionais) para um tipo de importa√ß√£o
        """
        from services.import_service import ImportService
        return ImportService.get_target_columns(import_type)
    
    def _create_prompt_process_transactions(
        self,
        file_data: str,
        columns: List[str],
        data_sample: str,
        is_pdf_source: bool = False,
        groups_subgroups: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """
        Cria prompt espec√≠fico para processar transa√ß√µes financeiras
        Baseado na estrutura real da tabela Transaction
        """
        pdf_note = ""
        if is_pdf_source:
            pdf_note = """
**NOTA IMPORTANTE - Arquivo PDF:**
- Os dados podem incluir texto completo do PDF, cabe√ßalhos, rodap√©s e metadados
- Use TODAS as informa√ß√µes dispon√≠veis (n√£o apenas tabelas) para identificar campos
- Se houver texto n√£o estruturado, analise-o cuidadosamente para encontrar dados relevantes
- Datas podem estar em diferentes formatos no texto - identifique e converta corretamente
"""
        
        groups_info = ""
        if groups_subgroups:
            groups_info = "\n**GRUPOS E SUBGRUPOS DISPON√çVEIS PARA CLASSIFICA√á√ÉO:**\n"
            for group in groups_subgroups:
                group_name = group.get('name', '')
                group_id = group.get('id', '')
                subgroups = group.get('subgroups', [])
                groups_info += f"- Grupo ID {group_id}: {group_name}\n"
                if subgroups:
                    for sg in subgroups:
                        sg_name = sg.get('name', '')
                        sg_id = sg.get('id', '')
                        groups_info += f"  - Subgrupo ID {sg_id}: {sg_name}\n"
            groups_info += "\n**IMPORTANTE - Classifica√ß√£o Autom√°tica:**\n"
            groups_info += "- Para CADA linha processada, analise a descri√ß√£o, categoria e valor\n"
            groups_info += "- Identifique o grupo e subgrupo mais apropriado baseado no contexto da transa√ß√£o\n"
            groups_info += "- Use group_id e subgroup_id nos dados processados\n"
            groups_info += "- Se n√£o conseguir identificar com certeza, deixe null\n"
        
        prompt = f"""Voc√™ √© um especialista em processamento de transa√ß√µes financeiras e cont√°beis.

Analise o arquivo de transa√ß√µes financeiras e estruture os dados para importa√ß√£o no sistema.

**Estrutura da Tabela de Transa√ß√µes:**
- date (Date, obrigat√≥rio): Data da transa√ß√£o no formato YYYY-MM-DD
- description (Text, obrigat√≥rio): Descri√ß√£o/hist√≥rico da transa√ß√£o
- value (Float, obrigat√≥rio): Valor da transa√ß√£o (n√∫mero decimal, sem s√≠mbolos)
- type (String, obrigat√≥rio): Tipo da transa√ß√£o - DEVE SER "entrada" ou "saida"
- category (String, opcional): Categoria da transa√ß√£o
- account (String, opcional): Conta/banco relacionado
- group_id (Integer, opcional): ID do grupo de classifica√ß√£o
- subgroup_id (Integer, opcional): ID do subgrupo de classifica√ß√£o

{groups_info}

{pdf_note}

**Colunas encontradas no arquivo:**
{', '.join(columns) if columns else 'Dados extra√≠dos do texto'}

**Amostra dos dados (primeiras 20 linhas):**
{data_sample}

**Dados completos (pode incluir texto, tabelas, metadados):**
{file_data}

**Regras de Processamento CR√çTICAS:**
1. DATAS: 
   - Use EXATAMENTE a data do arquivo original, linha por linha
   - Converta QUALQUER formato para YYYY-MM-DD
   - N√ÉO invente datas, use apenas as que est√£o no arquivo
   - Se houver m√∫ltiplas colunas de data, identifique qual √© a data da transa√ß√£o
   - Preserve a correspond√™ncia: original_row deve corresponder √† linha real do arquivo
   - Exemplos de convers√£o: "01/01/2024" ‚Üí "2024-01-01", "2024-01-01 10:30" ‚Üí "2024-01-01", "15-JAN-2024" ‚Üí "2024-01-15"

2. VALORES: Normalize valores monet√°rios para n√∫mero decimal
   - Remova s√≠mbolos: R$, BRL, $, etc
   - Remova pontos de milhar: 1.234,56 ‚Üí 1234.56
   - Mantenha v√≠rgula ou ponto como separador decimal: 1234,56 ou 1234.56
   - Valores negativos indicam SA√çDA, positivos indicam ENTRADA (se n√£o houver campo tipo expl√≠cito)

3. TIPO (entrada/saida): Identifique automaticamente
   - Se houver coluna expl√≠cita de tipo: use "entrada" ou "saida"
   - Se valor for negativo: "saida"
   - Se valor for positivo: "entrada"
   - Palavras-chave para sa√≠da: "d√©bito", "sa√≠da", "pagamento", "retirada", "transfer√™ncia enviada"
   - Palavras-chave para entrada: "cr√©dito", "entrada", "recebimento", "dep√≥sito", "transfer√™ncia recebida"

4. DESCRI√á√ÉO: Limpe e normalize
   - Remova espa√ßos extras
   - Mantenha informa√ß√µes relevantes
   - Se houver m√∫ltiplas colunas de descri√ß√£o, combine-as

5. GRUPO E SUBGRUPO (CLASSIFICA√á√ÉO PRINCIPAL - OBRIGAT√ìRIO):
   - **PRIORIDADE M√ÅXIMA:** Voc√™ DEVE analisar CADA linha e classificar por grupo e subgrupo PRIMEIRO
   - Analise a descri√ß√£o e valor de CADA transa√ß√£o
   - Compare com a lista de grupos e subgrupos fornecida acima
   - Identifique o grupo e subgrupo mais apropriado baseado no contexto
   - Use palavras-chave na descri√ß√£o para fazer a classifica√ß√£o:
     * "fornecedor", "compra", "pagamento" ‚Üí geralmente despesas/fornecedores
     * "sal√°rio", "folha", "funcion√°rio" ‚Üí geralmente despesas/pessoal
     * "aluguel", "energia", "√°gua", "telefone", "internet" ‚Üí geralmente despesas/opera√ß√£o
     * "venda", "recebimento", "cliente" ‚Üí geralmente receitas
     * "transfer√™ncia", "TED", "PIX" ‚Üí analise o contexto (entrada ou sa√≠da)
   - **SEMPRE retorne group_id e subgroup_id** - mesmo que seja null se n√£o conseguir identificar
   - Retorne os IDs num√©ricos (group_id e subgroup_id) nos dados processados
   - Se a descri√ß√£o n√£o for clara, use o valor e tipo (entrada/saida) para ajudar na classifica√ß√£o
   - **IMPORTANTE:** Grupo e subgrupo s√£o a classifica√ß√£o PRINCIPAL para relat√≥rios cont√°beis (DRE/DFC)

6. CATEGORIA (CLASSIFICA√á√ÉO SECUND√ÅRIA - OPCIONAL):
   - Use APENAS como classifica√ß√£o adicional/descritiva
   - Pode ser √∫til para an√°lises complementares ou filtros
   - Se n√£o conseguir identificar grupo/subgrupo, pode usar categoria como classifica√ß√£o gen√©rica
   - Exemplos: "alimenta√ß√£o", "transporte", "sal√°rio", "fornecedor", etc
   - **NOTA:** Categoria √© complementar, n√£o substitui grupo/subgrupo

7. CONTA: Identifique se houver informa√ß√£o de banco/conta

**Tarefa:**
Processe TODAS as linhas do arquivo e retorne dados estruturados prontos para importa√ß√£o, incluindo classifica√ß√£o autom√°tica por grupo e subgrupo.

**CR√çTICO - Correspond√™ncia de Linhas:**
- O campo "original_row" DEVE corresponder EXATAMENTE ao n√∫mero da linha no arquivo original
- Se o arquivo tem 10 linhas, original_row deve ir de 1 a 10
- Use o √≠ndice da linha no array de dados fornecido + 1 (primeira linha = 1, segunda = 2, etc)
- Isso √© ESSENCIAL para garantir que as datas correspondam corretamente

**IMPORTANTE - Regras para JSON v√°lido:**
- Retorne APENAS JSON v√°lido, sem texto adicional antes ou depois
- Escape corretamente caracteres especiais em strings:
  * Aspas duplas dentro de strings: use \"
  * Quebras de linha: use \\n (n√£o use quebras reais de linha)
  * Barras invertidas: use \\\\
- Limite descri√ß√µes a 500 caracteres (trunque se necess√°rio)
- Garanta que todas as strings estejam entre aspas duplas
- N√£o use aspas simples para strings
- Se uma descri√ß√£o contiver caracteres especiais, escape-os corretamente

**Responda em formato JSON v√°lido:**
{{
    "processed_data": [
        {{
            "date": "2024-01-15",
            "description": "Pagamento fornecedor ABC",
            "value": 1500.00,
            "type": "saida",
            "category": "fornecedor",
            "account": "Banco do Brasil",
            "group_id": 1,
            "subgroup_id": 3,
            "original_row": 1,
            "confidence": 0.95
        }}
    ],
    "summary": {{
        "total_rows": 100,
        "processed": 98,
        "errors": 2,
        "entradas": 45,
        "saidas": 53
    }},
    "issues": [
        "Linha 5: Data inv√°lida, usando data atual",
        "Linha 12: Valor n√£o num√©rico, ignorado"
    ]
}}
"""
        return prompt
    
    def _create_prompt_process_bank_statements(
        self,
        file_data: str,
        columns: List[str],
        data_sample: str,
        is_pdf_source: bool = False,
        groups_subgroups: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """
        Cria prompt espec√≠fico para processar extratos banc√°rios
        Baseado na estrutura real da tabela BankStatement
        """
        pdf_note = ""
        if is_pdf_source:
            pdf_note = """
**NOTA IMPORTANTE - Arquivo PDF:**
- Os dados podem incluir texto completo do PDF, cabe√ßalhos, rodap√©s e metadados
- Use TODAS as informa√ß√µes dispon√≠veis (n√£o apenas tabelas) para identificar campos
- Extraia nome do banco, n√∫mero de conta e outras informa√ß√µes dos cabe√ßalhos/rodap√©s
- Se houver texto n√£o estruturado, analise-o cuidadosamente para encontrar dados relevantes
- Datas podem estar em diferentes formatos no texto - identifique e converta corretamente
"""
        
        groups_info = ""
        if groups_subgroups:
            groups_info = "\n**GRUPOS E SUBGRUPOS DISPON√çVEIS PARA CLASSIFICA√á√ÉO:**\n"
            for group in groups_subgroups:
                group_name = group.get('name', '')
                group_id = group.get('id', '')
                subgroups = group.get('subgroups', [])
                groups_info += f"- Grupo ID {group_id}: {group_name}\n"
                if subgroups:
                    for sg in subgroups:
                        sg_name = sg.get('name', '')
                        sg_id = sg.get('id', '')
                        groups_info += f"  - Subgrupo ID {sg_id}: {sg_name}\n"
            groups_info += "\n**IMPORTANTE - Classifica√ß√£o Autom√°tica:**\n"
            groups_info += "- Para CADA linha processada, analise a descri√ß√£o/hist√≥rico e valor\n"
            groups_info += "- Identifique o grupo e subgrupo mais apropriado baseado no contexto da transa√ß√£o\n"
            groups_info += "- Use group_id e subgroup_id nos dados processados\n"
            groups_info += "- Se n√£o conseguir identificar com certeza, deixe null\n"
        
        prompt = f"""Voc√™ √© um especialista em processamento de extratos banc√°rios.

Analise o arquivo de extrato banc√°rio e estruture os dados para importa√ß√£o no sistema.

**Estrutura da Tabela de Extratos Banc√°rios:**
- date (Date, obrigat√≥rio): Data da transa√ß√£o no formato YYYY-MM-DD - DEVE SER EXATAMENTE A DATA DO ARQUIVO ORIGINAL
- description (Text, obrigat√≥rio): Descri√ß√£o/hist√≥rico da transa√ß√£o
- value (Float, obrigat√≥rio): Valor da transa√ß√£o (n√∫mero decimal, negativo para d√©bitos, positivo para cr√©ditos)
- bank_name (String, opcional): Nome do banco - EXTRAIA DO ARQUIVO (cabe√ßalho, rodap√©, ou padr√µes nas descri√ß√µes)
- account (String, opcional): N√∫mero da conta
- balance (Float, opcional): Saldo ap√≥s a transa√ß√£o
- group_id (Integer, opcional): ID do grupo de classifica√ß√£o
- subgroup_id (Integer, opcional): ID do subgrupo de classifica√ß√£o

{groups_info}

{pdf_note}

**Colunas encontradas no arquivo:**
{', '.join(columns) if columns else 'Dados extra√≠dos do texto'}

**Amostra dos dados (primeiras 20 linhas):**
{data_sample}

**Dados completos (pode incluir texto, tabelas, metadados):**
{file_data}

**Regras de Processamento CR√çTICAS:**
1. DATAS: 
   - Use EXATAMENTE a data do arquivo original, linha por linha
   - Converta QUALQUER formato para YYYY-MM-DD
   - N√ÉO invente datas, use apenas as que est√£o no arquivo
   - Se houver m√∫ltiplas colunas de data, use a coluna de data da transa√ß√£o
   - Preserve a correspond√™ncia: original_row deve corresponder √† linha real do arquivo

2. VALORES: Normalize para n√∫mero decimal (negativo = d√©bito, positivo = cr√©dito)
   - Mantenha o sinal original do arquivo

3. DESCRI√á√ÉO: Limpe e normalize hist√≥rico, mas mantenha informa√ß√µes importantes

4. NOME DO BANCO: 
   - EXTRAIA automaticamente do arquivo
   - Procure em: cabe√ßalhos, rodap√©s, nomes de colunas, descri√ß√µes, ou padr√µes conhecidos
   - Exemplos: "Banco do Brasil", "Ita√∫", "Bradesco", "Caixa", "Santander", "Nubank", etc
   - Se encontrar, use o mesmo nome para todas as linhas
   - Se n√£o encontrar, deixe vazio

5. GRUPO E SUBGRUPO (CLASSIFICA√á√ÉO PRINCIPAL - OBRIGAT√ìRIO):
   - **PRIORIDADE M√ÅXIMA:** Voc√™ DEVE analisar CADA linha e classificar por grupo e subgrupo PRIMEIRO
   - Analise a descri√ß√£o/hist√≥rico e valor de CADA transa√ß√£o
   - Compare com a lista de grupos e subgrupos fornecida acima
   - Identifique o grupo e subgrupo mais apropriado baseado no contexto
   - Use palavras-chave no hist√≥rico para fazer a classifica√ß√£o:
     * "TED", "PIX", "DOC", "TRANSFER√äNCIA" ‚Üí analise se √© entrada ou sa√≠da
     * "FORNECEDOR", "PAGAMENTO", "COMPRA" ‚Üí geralmente despesas/fornecedores
     * "SAL√ÅRIO", "FOLHA", "FUNCION√ÅRIO" ‚Üí geralmente despesas/pessoal
     * "ALUGUEL", "ENERGIA", "√ÅGUA", "TELEFONE" ‚Üí geralmente despesas/opera√ß√£o
     * "RECEBIMENTO", "CLIENTE", "VENDA" ‚Üí geralmente receitas
   - **SEMPRE retorne group_id e subgroup_id** - mesmo que seja null se n√£o conseguir identificar
   - Retorne os IDs num√©ricos (group_id e subgroup_id) nos dados processados
   - Se o hist√≥rico n√£o for claro, use o valor e sinal (positivo/negativo) para ajudar na classifica√ß√£o
   - **IMPORTANTE:** Grupo e subgrupo s√£o a classifica√ß√£o PRINCIPAL para relat√≥rios cont√°beis (DRE/DFC)

6. CONTA: Identifique n√∫mero da conta se houver

7. SALDO: Identifique se houver coluna de saldo

**CR√çTICO - Correspond√™ncia de Linhas:**
- O campo "original_row" DEVE corresponder EXATAMENTE ao n√∫mero da linha no arquivo original
- Se o arquivo tem 10 linhas, original_row deve ir de 1 a 10
- Use o √≠ndice da linha no array de dados fornecido + 1 (primeira linha = 1, segunda = 2, etc)
- Isso √© ESSENCIAL para garantir que as datas correspondam corretamente

**IMPORTANTE - Regras para JSON v√°lido:**
- Retorne APENAS JSON v√°lido, sem texto adicional
- Escape corretamente caracteres especiais em strings
- Limite descri√ß√µes a 500 caracteres

**Responda em formato JSON:**
{{
    "processed_data": [
        {{
            "date": "2024-01-15",
            "description": "TED RECEBIDA",
            "value": 5000.00,
            "bank_name": "Banco do Brasil",
            "account": "12345-6",
            "balance": 15000.00,
            "group_id": 1,
            "subgroup_id": 3,
            "original_row": 1
        }}
    ],
    "summary": {{
        "total_rows": 50,
        "processed": 50,
        "bank_name": "Banco do Brasil"
    }}
}}
"""
        return prompt
    
    def _create_prompt_process_accounts_payable(
        self,
        file_data: str,
        columns: List[str],
        data_sample: str,
        is_pdf_source: bool = False
    ) -> str:
        """
        Cria prompt espec√≠fico para processar contas a pagar
        """
        prompt = f"""Voc√™ √© um especialista em processamento de contas a pagar.

**Estrutura da Tabela:**
- account_name (String, obrigat√≥rio): Nome do credor/fornecedor
- due_date (Date, obrigat√≥rio): Data de vencimento YYYY-MM-DD
- value (Float, obrigat√≥rio): Valor a pagar
- cpf_cnpj (String, opcional): CPF/CNPJ
- month_ref (String, opcional): M√™s de refer√™ncia YYYY-MM
- paid (Boolean, opcional): Se j√° foi pago

**Colunas do arquivo:** {', '.join(columns)}
**Amostra:** {data_sample}
**Dados completos:** {file_data}

Processe e retorne em JSON com array "processed_data".
"""
        return prompt
    
    def _create_prompt_process_accounts_receivable(
        self,
        file_data: str,
        columns: List[str],
        data_sample: str,
        is_pdf_source: bool = False
    ) -> str:
        """
        Cria prompt espec√≠fico para processar contas a receber
        """
        prompt = f"""Voc√™ √© um especialista em processamento de contas a receber.

**Estrutura da Tabela:**
- account_name (String, obrigat√≥rio): Nome do devedor/cliente
- due_date (Date, obrigat√≥rio): Data de vencimento YYYY-MM-DD
- value (Float, obrigat√≥rio): Valor a receber
- cpf_cnpj (String, opcional): CPF/CNPJ
- month_ref (String, opcional): M√™s de refer√™ncia YYYY-MM
- received (Boolean, opcional): Se j√° foi recebido

**Colunas do arquivo:** {', '.join(columns)}
**Amostra:** {data_sample}
**Dados completos:** {file_data}

Processe e retorne em JSON com array "processed_data".
"""
        return prompt
    
    def _create_prompt_process_contracts(
        self,
        file_data: str,
        columns: List[str],
        data_sample: str,
        is_pdf_source: bool = False
    ) -> str:
        """
        Cria prompt espec√≠fico para processar contratos
        """
        pdf_note = ""
        if is_pdf_source:
            pdf_note = """
**NOTA IMPORTANTE - Arquivo PDF:**
- Os dados podem incluir texto completo do PDF, cabe√ßalhos, rodap√©s e metadados
- Use TODAS as informa√ß√µes dispon√≠veis para identificar campos
"""
        
        prompt = f"""Voc√™ √© um especialista em processamento de contratos e eventos.

Analise o arquivo e estruture os dados para importa√ß√£o.

**Estrutura esperada:**
- contract_start (Date): Data de in√≠cio do contrato
- event_date (Date): Data do evento
- service_value (Float): Valor do servi√ßo
- displacement_value (Float): Valor do deslocamento
- event_type (String): Tipo de evento
- service_sold (String): Servi√ßo vendido
- guests_count (Integer): N√∫mero de convidados
- contractor_name (String): Nome do contratante

{pdf_note}

**Colunas encontradas:** {', '.join(columns) if columns else 'Dados extra√≠dos do texto'}
**Amostra:** {data_sample}
**Dados completos:** {file_data}

Processe e retorne em JSON com array "processed_data".
"""
        return prompt
    
    def _create_prompt_process_financial_investments(
        self,
        file_data: str,
        columns: List[str],
        data_sample: str,
        is_pdf_source: bool = False
    ) -> str:
        """
        Cria prompt espec√≠fico para processar extratos de aplica√ß√µes financeiras
        """
        pdf_note = ""
        if is_pdf_source:
            pdf_note = """
**NOTA IMPORTANTE - Arquivo PDF:**
- Os dados podem incluir texto completo do PDF, cabe√ßalhos, rodap√©s e metadados
- Use TODAS as informa√ß√µes dispon√≠veis para identificar campos
"""
        
        prompt = f"""Voc√™ √© um especialista em processamento de extratos de aplica√ß√µes financeiras.

Analise o arquivo e estruture os dados para importa√ß√£o.

**Estrutura esperada:**
- date (Date): Data da opera√ß√£o
- investment_type (String): Tipo de aplica√ß√£o (CDB, LCI, LCA, Tesouro, etc)
- institution (String): Institui√ß√£o financeira
- operation_type (String): aplicado ou resgatado
- applied_value (Float): Valor aplicado
- redeemed_value (Float): Valor resgatado
- yield_value (Float): Rendimento
- balance (Float): Saldo atual

{pdf_note}

**Colunas encontradas:** {', '.join(columns) if columns else 'Dados extra√≠dos do texto'}
**Amostra:** {data_sample}
**Dados completos:** {file_data}

Processe e retorne em JSON com array "processed_data".
"""
        return prompt
    
    def _create_prompt_process_credit_card_invoices(
        self,
        file_data: str,
        columns: List[str],
        data_sample: str,
        is_pdf_source: bool = False
    ) -> str:
        """
        Cria prompt espec√≠fico para processar faturas de cart√£o de cr√©dito
        """
        pdf_note = ""
        if is_pdf_source:
            pdf_note = """
**NOTA IMPORTANTE - Arquivo PDF:**
- Os dados podem incluir texto completo do PDF, cabe√ßalhos, rodap√©s e metadados
- Use TODAS as informa√ß√µes dispon√≠veis para identificar campos
"""
        
        prompt = f"""Voc√™ √© um especialista em processamento de faturas de cart√£o de cr√©dito.

Analise o arquivo e estruture os dados para importa√ß√£o.

**Estrutura esperada:**
- transaction_date (Date): Data da transa√ß√£o
- description (String): Descri√ß√£o da transa√ß√£o
- value (Float): Valor
- category (String): Categoria
- establishment (String): Estabelecimento
- installment_number (Integer): N√∫mero da parcela
- total_installments (Integer): Total de parcelas
- card_brand (String): Bandeira do cart√£o

{pdf_note}

**Colunas encontradas:** {', '.join(columns) if columns else 'Dados extra√≠dos do texto'}
**Amostra:** {data_sample}
**Dados completos:** {file_data}

Processe e retorne em JSON com array "processed_data".
"""
        return prompt
    
    def _create_prompt_process_card_machine_statements(
        self,
        file_data: str,
        columns: List[str],
        data_sample: str,
        is_pdf_source: bool = False
    ) -> str:
        """
        Cria prompt espec√≠fico para processar extratos de m√°quina de cart√£o
        """
        pdf_note = ""
        if is_pdf_source:
            pdf_note = """
**NOTA IMPORTANTE - Arquivo PDF:**
- Os dados podem incluir texto completo do PDF, cabe√ßalhos, rodap√©s e metadados
- Use TODAS as informa√ß√µes dispon√≠veis para identificar campos
"""
        
        prompt = f"""Voc√™ √© um especialista em processamento de extratos de m√°quina de cart√£o.

Analise o arquivo e estruture os dados para importa√ß√£o.

**Estrutura esperada:**
- date (Date): Data da transa√ß√£o
- gross_value (Float): Valor bruto
- fee (Float): Taxa cobrada
- net_value (Float): Valor l√≠quido
- card_brand (String): Bandeira do cart√£o (Visa, Mastercard, Elo, etc)
- transaction_type (String): debito ou credito
- description (String): Descri√ß√£o

{pdf_note}

**Colunas encontradas:** {', '.join(columns) if columns else 'Dados extra√≠dos do texto'}
**Amostra:** {data_sample}
**Dados completos:** {file_data}

Processe e retorne em JSON com array "processed_data".
"""
        return prompt
    
    def _create_prompt_process_inventory(
        self,
        file_data: str,
        columns: List[str],
        data_sample: str,
        is_pdf_source: bool = False
    ) -> str:
        """
        Cria prompt espec√≠fico para processar controle de estoque
        """
        pdf_note = ""
        if is_pdf_source:
            pdf_note = """
**NOTA IMPORTANTE - Arquivo PDF:**
- Os dados podem incluir texto completo do PDF, cabe√ßalhos, rodap√©s e metadados
- Use TODAS as informa√ß√µes dispon√≠veis para identificar campos
"""
        
        prompt = f"""Voc√™ √© um especialista em processamento de controle de estoque.

Analise o arquivo e estruture os dados para importa√ß√£o.

**Estrutura esperada:**
- product_name (String): Nome do produto
- quantity (Float): Quantidade (pode ser decimal)
- unit_value (Float): Valor unit√°rio
- movement_date (Date): Data do movimento
- movement_type (String): entrada ou saida
- description (String): Descri√ß√£o

{pdf_note}

**Colunas encontradas:** {', '.join(columns) if columns else 'Dados extra√≠dos do texto'}
**Amostra:** {data_sample}
**Dados completos:** {file_data}

Processe e retorne em JSON com array "processed_data".
"""
        return prompt
    
    def process_and_structure_data(
        self,
        df: pd.DataFrame,
        import_type: str,
        pdf_full_data: Optional[Dict[str, Any]] = None,
        groups_subgroups: Optional[List[Dict[str, Any]]] = None,
        status_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        Processa arquivo completo com IA e retorna dados estruturados prontos para importa√ß√£o
        
        Args:
            df: DataFrame com os dados do arquivo
            import_type: Tipo de importa√ß√£o (transactions, bank_statements, etc)
            pdf_full_data: Dados completos do PDF (opcional)
            groups_subgroups: Lista de grupos e subgrupos para classifica√ß√£o autom√°tica (opcional)
            status_callback: Fun√ß√£o callback(status_message) para atualizar status em tempo real (opcional)
        
        Retorna:
        {
            'success': bool,
            'processed_data': List[Dict],  # Dados processados
            'summary': Dict,  # Estat√≠sticas
            'issues': List[str],  # Problemas encontrados
            'error': str  # Erro se houver
        }
        """
        if not self.is_available():
            return {
                'success': False,
                'error': 'IA n√£o configurada ou n√£o dispon√≠vel',
                'processed_data': [],
                'summary': {},
                'issues': []
            }
        
        try:
            if status_callback:
                status_callback("Analisando estrutura do arquivo...")
            
            # Se for PDF e tiver dados completos, usa informa√ß√µes adicionais
            pdf_context = ""
            if pdf_full_data:
                pdf_context = self._prepare_pdf_context(pdf_full_data, import_type)
            
            # Se DataFrame estiver vazio mas tiver dados de PDF, cria DataFrame do texto
            if df.empty and pdf_full_data and pdf_full_data.get('full_text'):
                # Tenta criar DataFrame b√°sico do texto para processamento
                df = self._text_to_dataframe(pdf_full_data.get('full_text'))
            
            columns = list(df.columns) if not df.empty else []
            
            if status_callback:
                status_callback(f"Preparando dados para processamento ({len(df)} linhas encontradas)...")
            
            # Prepara amostra e dados completos
            # Processa TODO o arquivo, n√£o apenas uma amostra limitada
            is_pdf = pdf_full_data is not None
            max_rows_sample = 30 if is_pdf else 20  # Amostra apenas para exibi√ß√£o no prompt
            
            # Processa TODAS as linhas do DataFrame
            if not df.empty:
                data_sample = self._prepare_data_sample(df, max_rows=max_rows_sample)
                # Usa TODO o DataFrame, n√£o apenas uma amostra
                file_data_df = df.copy()
                file_data_df['_original_index'] = file_data_df.index
            else:
                data_sample = "Dados extra√≠dos do texto do PDF"
                file_data_df = pd.DataFrame()
            
            # Prepara metadados e contexto
            file_metadata = ""
            if import_type == 'bank_statements' or is_pdf:
                metadata_parts = []
                
                if columns:
                    metadata_parts.append(f"- Nome das colunas: {', '.join(columns)}")
                
                if not df.empty:
                    metadata_parts.append(f"- Primeiras 3 linhas do arquivo:\n{df.head(3).to_string()}")
                    if len(df) > 3:
                        metadata_parts.append(f"- √öltimas 3 linhas do arquivo:\n{df.tail(3).to_string()}")
                
                if pdf_context:
                    metadata_parts.append(pdf_context)
                
                if metadata_parts:
                    file_metadata = "**Informa√ß√µes adicionais do arquivo:**\n" + "\n".join(metadata_parts) + "\n"
            
            # Prepara dados para JSON
            if not df.empty:
                # Informa quantidade total de linhas no metadata
                total_rows = len(file_data_df)
                if status_callback:
                    status_callback(f"Processando {total_rows} linhas do arquivo...")
                
                file_data = json.dumps(
                    file_data_df.to_dict('records'),
                    indent=2,
                    default=str,
                    ensure_ascii=False
                )
            else:
                # Se n√£o tem DataFrame, usa texto completo do PDF (SEM limita√ß√£o)
                if pdf_full_data and pdf_full_data.get('full_text'):
                    full_text = pdf_full_data.get('full_text', '')
                    # Usa TODO o texto, n√£o apenas 10k chars
                    file_data = full_text
                    if status_callback:
                        status_callback(f"Processando texto completo do PDF ({len(full_text)} caracteres)...")
                else:
                    file_data = ''
            
            # Adiciona metadados ao file_data
            if file_metadata:
                file_data = file_metadata + "\n**Dados do arquivo:**\n" + file_data
            
            if status_callback:
                status_callback("Criando prompt de processamento...")
            
            # Seleciona prompt baseado no tipo
            # Para PDFs, inclui flag indicando que h√° contexto adicional
            is_pdf_source = pdf_full_data is not None
            
            if import_type == 'transactions':
                prompt = self._create_prompt_process_transactions(
                    file_data, columns, data_sample, 
                    is_pdf_source=is_pdf_source,
                    groups_subgroups=groups_subgroups
                )
            elif import_type == 'bank_statements':
                prompt = self._create_prompt_process_bank_statements(
                    file_data, columns, data_sample, 
                    is_pdf_source=is_pdf_source,
                    groups_subgroups=groups_subgroups
                )
            elif import_type == 'contracts':
                prompt = self._create_prompt_process_contracts(file_data, columns, data_sample, is_pdf_source=is_pdf_source)
            elif import_type == 'accounts_payable':
                prompt = self._create_prompt_process_accounts_payable(file_data, columns, data_sample, is_pdf_source=is_pdf_source)
            elif import_type == 'accounts_receivable':
                prompt = self._create_prompt_process_accounts_receivable(file_data, columns, data_sample, is_pdf_source=is_pdf_source)
            elif import_type == 'financial_investments':
                prompt = self._create_prompt_process_financial_investments(file_data, columns, data_sample, is_pdf_source=is_pdf_source)
            elif import_type == 'credit_card_invoices':
                prompt = self._create_prompt_process_credit_card_invoices(file_data, columns, data_sample, is_pdf_source=is_pdf_source)
            elif import_type == 'card_machine_statements':
                prompt = self._create_prompt_process_card_machine_statements(file_data, columns, data_sample, is_pdf_source=is_pdf_source)
            elif import_type == 'inventory':
                prompt = self._create_prompt_process_inventory(file_data, columns, data_sample, is_pdf_source=is_pdf_source)
            else:
                return {
                    'success': False,
                    'error': f'Tipo de importa√ß√£o n√£o suportado: {import_type}',
                    'processed_data': [],
                    'summary': {},
                    'issues': []
                }
            
            if status_callback:
                status_callback("Classificando por grupo e subgrupo...")
            
            # Chama IA
            response, error = self._call_ai(prompt, status_callback=status_callback)
            
            if error:
                if status_callback:
                    status_callback(f"‚ùå Erro: {error}")
                return {
                    'success': False,
                    'error': error,
                    'processed_data': [],
                    'summary': {},
                    'issues': []
                }
            
            if not response:
                if status_callback:
                    status_callback("‚ùå Sem resposta da IA")
                return {
                    'success': False,
                    'error': 'Sem resposta da IA',
                    'processed_data': [],
                    'summary': {},
                    'issues': []
                }
            
            if status_callback:
                status_callback("Processando resposta da IA...")
            
            # Parse da resposta
            try:
                # Remove markdown code blocks se existirem
                if '```json' in response:
                    response = response.split('```json')[1].split('```')[0]
                elif '```' in response:
                    response = response.split('```')[1].split('```')[0]
                
                # Limpa a resposta
                response_clean = response.strip()
                
                # Tenta encontrar o JSON v√°lido na resposta
                # Procura pelo primeiro { e √∫ltimo }
                start_idx = response_clean.find('{')
                if start_idx == -1:
                    raise json.JSONDecodeError("JSON n√£o encontrado na resposta", response_clean, 0)
                
                # Procura o √∫ltimo } v√°lido (pode haver m√∫ltiplos objetos)
                end_idx = response_clean.rfind('}')
                if end_idx == -1 or end_idx <= start_idx:
                    raise json.JSONDecodeError("JSON incompleto", response_clean, start_idx)
                
                # Extrai o JSON
                json_str = response_clean[start_idx:end_idx + 1]
                
                # Tenta parsear
                try:
                    result = json.loads(json_str)
                except json.JSONDecodeError as e:
                    # Se falhar, tenta reparar strings n√£o terminadas
                    json_str_clean = json_str
                    
                    # Remove quebras de linha dentro de strings (exceto \n escapado)
                    json_str_clean = re.sub(r'(?<!\\)\n', ' ', json_str_clean)
                    json_str_clean = re.sub(r'(?<!\\)\r', ' ', json_str_clean)
                    json_str_clean = re.sub(r'(?<!\\)\t', ' ', json_str_clean)
                    
                    # Tenta encontrar e fechar strings n√£o terminadas
                    # Procura por padr√£o: "texto sem fechamento
                    # Adiciona " antes de caracteres problem√°ticos
                    in_string = False
                    escape_next = False
                    result_chars = []
                    
                    for i, char in enumerate(json_str_clean):
                        if escape_next:
                            result_chars.append(char)
                            escape_next = False
                            continue
                        
                        if char == '\\':
                            result_chars.append(char)
                            escape_next = True
                            continue
                        
                        if char == '"':
                            in_string = not in_string
                            result_chars.append(char)
                        elif in_string:
                            # Dentro de string, substitui caracteres problem√°ticos
                            if char in ['\n', '\r', '\t']:
                                result_chars.append(' ')
                            elif char == '\x00':  # Null bytes
                                result_chars.append(' ')
                            else:
                                result_chars.append(char)
                        else:
                            result_chars.append(char)
                    
                    # Se ainda estiver em string no final, fecha ela
                    if in_string:
                        result_chars.append('"')
                    
                    json_str_clean = ''.join(result_chars)
                    
                    try:
                        result = json.loads(json_str_clean)
                    except json.JSONDecodeError as e2:
                        # Tenta reparar problemas comuns de JSON
                        json_str_final = self._repair_json(json_str_clean, e2)
                        try:
                            result = json.loads(json_str_final)
                        except json.JSONDecodeError as e3:
                            # √öltima tentativa: extrai apenas o que √© poss√≠vel parsear
                            result = self._extract_partial_json(json_str_clean)
                            if not result:
                                # Se ainda falhar, levanta o erro com contexto
                                raise e2
                
                # Se processou apenas amostra, aplica padr√µes ao resto
                processed_data = result.get('processed_data', [])
                
                # Valida e corrige datas usando os dados originais do arquivo
                # Garante que as datas correspondam exatamente ao arquivo original
                from utils.validators import parse_date
                
                # Identifica coluna de data no arquivo original
                date_col = None
                for col in file_data_df.columns:
                    if col == '_original_index':
                        continue
                    col_lower = str(col).lower().strip()
                    if any(keyword in col_lower for keyword in ['data', 'date', 'dt', 'transacao', 'lancamento', 'vencimento', 'dia']):
                        date_col = col
                        break
                
                # Se encontrou coluna de data, for√ßa o uso das datas originais
                if date_col:
                    for idx, item in enumerate(processed_data):
                        # Tenta usar original_row, mas se n√£o corresponder, usa o √≠ndice do array
                        original_row = item.get('original_row', 0)
                        original_idx = original_row - 1 if original_row > 0 else idx
                        
                        # Garante que o √≠ndice esteja dentro do range
                        if original_idx < 0 or original_idx >= len(file_data_df):
                            original_idx = idx
                        
                        # Se o √≠ndice estiver dentro do range processado, usa a data original
                        if 0 <= original_idx < len(file_data_df):
                            original_row_data = file_data_df.iloc[original_idx]
                            
                            if date_col in original_row_data:
                                original_date = str(original_row_data[date_col])
                                if original_date and original_date != 'nan' and original_date.strip() and original_date.lower() not in ['none', 'null', '']:
                                    try:
                                        # Parse da data original - FOR√áA o uso
                                        parsed_original = parse_date(original_date)
                                        if parsed_original:
                                            # FOR√áA o uso da data original parseada
                                            item['date'] = parsed_original.strftime('%Y-%m-%d')
                                    except Exception as e:
                                        # Se n√£o conseguir parsear, mant√©m a data processada pela IA
                                        pass
                
                # Remove _original_index dos dados processados se existir
                for item in processed_data:
                    item.pop('_original_index', None)
                
                # Se houver mais linhas, processa o resto com padr√µes identificados
                if max_rows_to_process < len(df):
                    # Por enquanto, retorna apenas as processadas
                    # Em produ√ß√£o, poderia aplicar transforma√ß√µes identificadas
                    pass
                
                if status_callback:
                    status_callback(f"‚úÖ Processamento conclu√≠do! {len(processed_data)} linhas processadas.")
                
                return {
                    'success': True,
                    'processed_data': processed_data,
                    'summary': result.get('summary', {}),
                    'issues': result.get('issues', []),
                    'error': None
                }
                
            except json.JSONDecodeError as e:
                # Tenta extrair informa√ß√µes √∫teis do erro
                error_pos = getattr(e, 'pos', None)
                error_line = getattr(e, 'lineno', None)
                error_col = getattr(e, 'colno', None)
                
                error_msg = f'Erro ao parsear resposta da IA: {str(e)}'
                if error_line and error_col:
                    error_msg += f' (linha {error_line}, coluna {error_col})'
                
                # Mostra contexto do erro
                if error_pos and error_pos < len(response):
                    start = max(0, error_pos - 100)
                    end = min(len(response), error_pos + 100)
                    context = response[start:end]
                    error_msg += f'\nContexto: ...{context}...'
                
                if status_callback:
                    status_callback(f"‚ùå Erro ao processar resposta: {error_msg}")
                
                return {
                    'success': False,
                    'error': error_msg,
                    'processed_data': [],
                    'summary': {},
                    'issues': [],
                    'raw_response': response[:2000] if len(response) > 2000 else response
                }
                
        except Exception as e:
            if status_callback:
                status_callback(f"‚ùå Erro no processamento: {str(e)}")
            return {
                'success': False,
                'error': f'Erro ao processar dados: {str(e)}',
                'processed_data': [],
                'summary': {},
                'issues': []
            }

    def analyze_structure(
        self,
        df: pd.DataFrame,
        import_type: str
    ) -> Dict[str, Any]:
        """
        Realiza an√°lise estrutural completa do arquivo
        """
        if not self.is_available():
            return {}
        
        try:
            columns = list(df.columns)
            data_sample = self._prepare_data_sample(df, max_rows=10)
            prompt = self._create_prompt_structural_analysis(columns, data_sample, import_type)
            
            response, error = self._call_ai(prompt)
            
            if error:
                print(f"Erro na an√°lise estrutural: {error}")
                return {}
            
            if response:
                try:
                    if '```json' in response:
                        response = response.split('```json')[1].split('```')[0]
                    elif '```' in response:
                        response = response.split('```')[1].split('```')[0]
                    
                    return json.loads(response.strip())
                except json.JSONDecodeError as e:
                    print(f"Erro ao parsear an√°lise estrutural: {e}")
                    return {}
        except Exception as e:
            print(f"Erro na an√°lise estrutural: {e}")
        
        return {}

    def intelligent_mapping(
        self,
        df: pd.DataFrame,
        import_type: str,
        structural_analysis: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Realiza mapeamento inteligente com contexto
        """
        if not self.is_available():
            return {}
        
        try:
            columns = list(df.columns)
            data_sample = self._prepare_data_sample(df, max_rows=15)
            analysis_str = json.dumps(structural_analysis, indent=2, ensure_ascii=False) if structural_analysis else None
            prompt = self._create_prompt_intelligent_mapping(columns, data_sample, import_type, analysis_str)
            
            response, error = self._call_ai(prompt)
            
            if error:
                print(f"Erro no mapeamento inteligente: {error}")
                return {}
            
            if response:
                try:
                    if '```json' in response:
                        response = response.split('```json')[1].split('```')[0]
                    elif '```' in response:
                        response = response.split('```')[1].split('```')[0]
                    
                    return json.loads(response.strip())
                except json.JSONDecodeError as e:
                    print(f"Erro ao parsear mapeamento inteligente: {e}")
                    return {}
        except Exception as e:
            print(f"Erro no mapeamento inteligente: {e}")
        
        return {}

    def normalize_data(
        self,
        df: pd.DataFrame,
        import_type: str,
        mapping: Dict[str, str],
        structural_analysis: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Normaliza e estrutura os dados
        """
        if not self.is_available():
            return {}
        
        try:
            # Prepara dados (primeiras 20 linhas)
            sample_df = df.head(20)
            file_data = json.dumps(sample_df.to_dict('records'), indent=2, default=str, ensure_ascii=False)
            analysis_str = json.dumps(structural_analysis, indent=2, ensure_ascii=False) if structural_analysis else "{}"
            
            prompt = self._create_prompt_normalization(file_data, import_type, analysis_str, mapping)
            
            response, error = self._call_ai(prompt)
            
            if error:
                print(f"Erro na normaliza√ß√£o: {error}")
                return {}
            
            if response:
                try:
                    if '```json' in response:
                        response = response.split('```json')[1].split('```')[0]
                    elif '```' in response:
                        response = response.split('```')[1].split('```')[0]
                    
                    return json.loads(response.strip())
                except json.JSONDecodeError as e:
                    print(f"Erro ao parsear normaliza√ß√£o: {e}")
                    return {}
        except Exception as e:
            print(f"Erro na normaliza√ß√£o: {e}")
        
        return {}

    def validate_data(
        self,
        normalized_data: List[Dict[str, Any]],
        import_type: str
    ) -> Dict[str, Any]:
        """
        Valida e corrige dados normalizados
        """
        if not self.is_available():
            return {}
        
        try:
            # Limita a 50 registros para n√£o exceder tokens
            data_to_validate = normalized_data[:50]
            data_str = json.dumps(data_to_validate, indent=2, ensure_ascii=False)
            
            prompt = self._create_prompt_validation(data_str, import_type)
            
            response, error = self._call_ai(prompt)
            
            if error:
                print(f"Erro na valida√ß√£o: {error}")
                return {}
            
            if response:
                try:
                    if '```json' in response:
                        response = response.split('```json')[1].split('```')[0]
                    elif '```' in response:
                        response = response.split('```')[1].split('```')[0]
                    
                    return json.loads(response.strip())
                except json.JSONDecodeError as e:
                    print(f"Erro ao parsear valida√ß√£o: {e}")
                    return {}
        except Exception as e:
            print(f"Erro na valida√ß√£o: {e}")
        
        return {}

    def infer_fields(
        self,
        df: pd.DataFrame,
        import_type: str,
        missing_fields: List[str],
        context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Infere campos faltantes
        """
        if not self.is_available():
            return {}
        
        try:
            # Prepara amostra de dados
            sample_df = df.head(20)
            available_data = json.dumps(sample_df.to_dict('records'), indent=2, default=str, ensure_ascii=False)
            
            prompt = self._create_prompt_inference(available_data, import_type, missing_fields, context)
            
            response, error = self._call_ai(prompt)
            
            if error:
                print(f"Erro na infer√™ncia: {error}")
                return {}
            
            if response:
                try:
                    if '```json' in response:
                        response = response.split('```json')[1].split('```')[0]
                    elif '```' in response:
                        response = response.split('```')[1].split('```')[0]
                    
                    return json.loads(response.strip())
                except json.JSONDecodeError as e:
                    print(f"Erro ao parsear infer√™ncia: {e}")
                    return {}
        except Exception as e:
            print(f"Erro na infer√™ncia: {e}")
        
        return {}

